{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riacode/StanfordAI4ALL/blob/main/Complete_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLTp_tYi8O2t"
      },
      "source": [
        "# Regression\n",
        "\n",
        "---\n",
        "Learning Goals:\n",
        "\n",
        "\n",
        "*   Implement a logistic regression classifier\n",
        "*   Compute the accuracy of a model on the test data; compare to accuracy on training data\n",
        "*   Implement Welch's T-Test to find significant genes\n",
        "*   Update the logistic regression model including significant genes\n",
        "\n",
        "AI4ALL Camp Day 7\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLoEpzGezYlS"
      },
      "source": [
        "#Import some useful things to get started\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression #These are the regression classifiers in the sklearn library\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z59HHaAXlKXc"
      },
      "source": [
        "#Part 1 -- Initial Logistic Regression\n",
        "\n",
        "Let's try out regression for our cancer data. Note that our predictor variables are continuous (the gene expression) and the variable we want to predict (the label) is binary. Thus, we should use logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ym42wdXleOj",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "9a262863-bc74-49d7-c72d-925b63164608"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a7b94a5d-01e4-475c-907d-1e074bdb5cdb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a7b94a5d-01e4-475c-907d-1e074bdb5cdb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving leukemia_ALL_AML.csv to leukemia_ALL_AML.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VA3BF1nlxWB"
      },
      "source": [
        "data = pd.read_csv(io.BytesIO(uploaded['leukemia_ALL_AML.csv']), encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcwEDZ_OdIEh"
      },
      "source": [
        "Let's start the logistic classifier and train it on the data. First, we will split the data into a training and test set. What do you think is a good size split?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXDi6gjQobq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a5c2147-d438-4dc2-83db-050b26b52bef"
      },
      "source": [
        "#First we need to remove the 'label' column from the dataset\n",
        "#Why do we need to do this?\n",
        "X = data.drop('label', axis = 1)\n",
        "y = data.label\n",
        "\n",
        "# Partition the dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=0) #here train_size is the proportion of data to put in the training set\n",
        "\n",
        "#How many entries are in the training data? How many in the test data?\n",
        "size_test = X_test.shape[0] #FILL IN\n",
        "size_train = X_train.shape[0] #FILL IN\n",
        "print(size_train, size_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36 36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEnZIHB-0wEC"
      },
      "source": [
        "Now let's initiate the classifier and train it on the training data. Here we're using the LogisticRegression class built in to our favorite library sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd5uVDr302hg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ea8cb0-7d63-48c0-802a-b91406b1e4a9"
      },
      "source": [
        "# Instantiate the classifier\n",
        "# The parameters here are the default values for LogisticRegression,\n",
        "# don't worry about them\n",
        "log_clf = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter = 100)\n",
        "\n",
        "# Train the classifier\n",
        "log_clf.fit(X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4offvsCTxrL"
      },
      "source": [
        "Remember that logistic regression is fitting a function to the data that outputs a probability (a number between 0 and 1). Let's look and see what these predicted values are for the training data.  Print out the probabilities -- what do you notice about the values?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHnMsljjT2eG"
      },
      "source": [
        "train_probs = log_clf.predict_proba(X_train)[:,1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ovw3eczU4zZ"
      },
      "source": [
        "Now let's classify the test data using the numbers. That is, we want to label everything with predicted probability > threshold as 1, and everything else as 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePzoocu9U3mJ"
      },
      "source": [
        "threshold = 0.5\n",
        "\n",
        "test_probs = log_clf.predict_proba(X_test)[:,1] #the predicted probabiilities for the test data\n",
        "\n",
        "predicted_labels = [] #This will contain 0s and 1s\n",
        "\n",
        "#One way of doing this is a loop, but feel free to use whatever method you like!\n",
        "for p in test_probs:\n",
        "  #Convert the value p (a number between 0 and 1) to either 0 or 1\n",
        "  if p > threshold:\n",
        "    predicted_labels.append(1)\n",
        "  else:\n",
        "    predicted_labels.append(0)\n",
        "\n",
        "# a fancier way\n",
        "# predicted_labels = (test_probs > threshold).astype(int)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUvaKzXqVHJp"
      },
      "source": [
        "The classifier can also give you the labels directly! (Using default threshold = 0.5) Print these out and compare to your result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Xpta-0WVFi1"
      },
      "source": [
        "test_preds = log_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY3odst3mkkC"
      },
      "source": [
        "We can do a quick plot to visualize how many the logistic regression correctly predicted. We can just plot the true value minus the prediction -- so if it's equal to 0, then the prediction was correct. If it's equal to 1 or -1 then the prediction was incorrect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IPbUX_4mLt5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "4d938c8e-4430-4cda-f7b8-4165eecf4793"
      },
      "source": [
        "\n",
        "# Plot the logistic regression predictions\n",
        "# 'm^' means magenta is the color, triangle is the shape\n",
        "plt.plot(y_test - test_preds, 'm^')\n",
        "\n",
        "# Label axes and legend\n",
        "plt.xlabel(\"Sample\")\n",
        "plt.ylabel(\"Prediction - Actual label\")\n",
        "#plt.legend(('Predicted - Actual'), loc=\"lower right\", fontsize='small')\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pmb4u2vmwqB"
      },
      "source": [
        "\n",
        "Can you see how many we got inccorect? Let's calculate the percent accuracy (the proportion correctly predicted) for the logistic regression. Fill in the below function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMWykaNrrYMH"
      },
      "source": [
        "#Let's make a general function\n",
        "def accuracy(predicted, actual):\n",
        "    '''\n",
        "    predicted: array of the predicted values (0 or 1)\n",
        "    actual: array of the real values (0 or 1)\n",
        "    '''\n",
        "    #FILL IN\n",
        "    actual = np.array(actual)\n",
        "    predicted = np.array(predicted)\n",
        "    num_correct = 0\n",
        "    num_total = predicted.shape[0]\n",
        "    for i in range(num_total):\n",
        "      if predicted[i] == actual[i]:\n",
        "        num_correct += 1\n",
        "\n",
        "    # a fancier way\n",
        "    # num_correct = (actual == predicted).sum()\n",
        "\n",
        "    accuracy = num_correct / num_total\n",
        "\n",
        "    return accuracy\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAJ9hhd96rJc"
      },
      "source": [
        "Calculate the accuracy on both the test set and the training set. What do you think it will be for the training test?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlcBmu8I6xJH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "668a505c-b3b6-4d9a-925b-c16c916dddc3"
      },
      "source": [
        "\n",
        "#Accuracy for the test set\n",
        "print(accuracy(test_preds, y_test))\n",
        "\n",
        "train_preds = log_clf.predict(X_train)\n",
        "#Accuracy for the training set\n",
        "print(accuracy(train_preds, y_train))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9722222222222222\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHhS5XiS5c2s"
      },
      "source": [
        "We can also use the built-in function \"score\". Use it to check your accuracy function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "The9M6-I5o40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f5d670d-2b1a-4c91-95b7-9c1615ae3736"
      },
      "source": [
        "log_clf.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9722222222222222"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoFGX9WjAvgG"
      },
      "source": [
        "Compare with classmate's result. Do you get the same numbers?\n",
        "\n",
        "Things to explore on your own if you want:\n",
        "\n",
        "1. Change the split size of training/test and see what happens\n",
        "2. See how accuracy changes with different threshold values -- But only see how the accuracy on the *training* data is changed. Remember that we want to keep the test data separate until we've finalized our model. If we use the test data to change our model, that's cheating!\n",
        "3. Compute false positive/false negatives and make the confusion matrix (we'll do this more in future days)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdgC8a5GsY_v"
      },
      "source": [
        "#Part 2 -- Gene Selection\n",
        "\n",
        "The more genes we include in the model, the better the results will be on the training set. However, we don't want to over-fit. We saw that the model's accuracy on the training set was close to 1.0 -- what does this mean?\n",
        "\n",
        "So, how do we decrease the number of features and decide what genes to include? One way is to do a hypothesis test on each gene to determine if there is a statistically significant difference between the two groups. Then, only include the genes for which there is a significant difference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne8f7JL8_g95"
      },
      "source": [
        "###Welch's T-Test\n",
        "\n",
        "Remember the formula for the t-statistic: We have two groups of data. $X_1, X_2$.\n",
        "* $\\overline{x_1}$ is the mean of the first group, $\\overline{x_2}$ is the mean of the second group\n",
        "* $s_1$ and $s_2$ are the standard deviations of each group\n",
        "* $n_1$ is the number of points in group $X_1$, $n_2$ is the number of points in group $X_2$\n",
        "\n",
        "The t-statistic is\n",
        "$$\n",
        "t = \\frac{\\overline{x_1} -  \\overline{x_2}}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n",
        "$$\n",
        "\n",
        "We also need the degree of freedom. The formula is\n",
        "$$\n",
        "df = \\frac{\\left( \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\right)^2}{\\frac{s_1^4}{n_1^2 \\cdot (n_1 - 1)} + \\frac{s_2^4}{n_2^2 \\cdot (n_2 - 1)}}\n",
        "$$\n",
        "\n",
        "(REMARK: Try to write the formula for coding practice, but there is always a built-in function you can use.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXm6gpRt-C3M"
      },
      "source": [
        "def welch_test(X1, X2):\n",
        "    '''\n",
        "    X1: array of data points\n",
        "    X2: array of data points\n",
        "    NOTE X and Y do not have to be the same length\n",
        "    '''\n",
        "    #-------------First, let's calculate all of the variables you'll need:\n",
        "\n",
        "    #the mean of each group\n",
        "    m1 = np.mean(X1)\n",
        "    m2 = np.mean(X2)\n",
        "\n",
        "    #The standard deviations of each group\n",
        "    s1 = np.std(X1)\n",
        "    s2 = np.std(X2)\n",
        "\n",
        "    #The number of samples in in each group\n",
        "    n1 = X1.shape[0]\n",
        "    n2 = X2.shape[0]\n",
        "\n",
        "    #-------------Now, calculate the t-statistic\n",
        "    #Calculate the numerator and denominator\n",
        "    t_num = m1 - m2\n",
        "    t_denom = (s1**2/n1 + s2**2/n2)**0.5\n",
        "\n",
        "    t_stat = t_num/t_denom\n",
        "\n",
        "    #-------------Finally, calculate the degree of freedom\n",
        "    df_num = (s1**2/n1 + s2**2/n2)**2\n",
        "    df_denom = s1**4/(n1**3-n1**2) + s2**4/(n2**3-n2**2)\n",
        "\n",
        "    df = df_num/df_denom\n",
        "\n",
        "    #Return both values\n",
        "    return([t_stat, df])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub5IjDmQ-n_B"
      },
      "source": [
        "Now let's loop through the genes and do the t-test comparing the groups. From the t-statistic and the degree of freedom, we can use a built-in function to get the p-value.  \n",
        "\n",
        "NOTE: If you're short on time, or frustrated with all the algebra, you can use the ttest function from scipy.stats. This will give you the p-value directly:      \n",
        "\n",
        "```\n",
        "p_value = ttest_ind(X, Y, equal_var = False)[1]\n",
        "```\n",
        "This is included as a commented line in the code below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td2jE9EKslyc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19603c26-0273-437d-d202-d128428aa7e2"
      },
      "source": [
        "from scipy.special import stdtr\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "\n",
        "# First let's get the number of genes we have to loop through\n",
        "number_of_genes = data.shape[1] - 1 #FILL IN\n",
        "\n",
        "selected_genes = [] #This list will store the genes we decide are significant\n",
        "\n",
        "for gene_id in range(number_of_genes):\n",
        "\n",
        "    expressions = data.iloc[:, gene_id]\n",
        "\n",
        "    #Get all oft the label0 values for this gene, and all the label1 values\n",
        "    label0 = expressions[data.label == 0]\n",
        "    label1 = expressions[data.label == 1]\n",
        "\n",
        "    result = welch_test(label0, label1)\n",
        "    t_stat = result[0]\n",
        "    df = result[1]\n",
        "\n",
        "    # now we can use an existing funtion to find the p-value (don't worry about this!)\n",
        "    # it is same as looking-up the t-test table!\n",
        "    p_value = 2*stdtr(df, -np.abs(t_stat))\n",
        "\n",
        "    #Un-comment this line if you want to use the built-in function\n",
        "    # p_value = ttest_ind(label0, label1, equal_var = False)[1]\n",
        "\n",
        "    #FILL IN -- write an if-statement to check if the p_value is less than 0.05. If so, add it to our selected_genes list\n",
        "    #Recall that to add to a list, you can do \".append(item)\"\n",
        "    if p_value < 0.05:\n",
        "      selected_genes.append(gene_id)\n",
        "\n",
        "\n",
        "#Print out the number of genes we selected\n",
        "print ('Number of informative genes: ', len(selected_genes)) #FILL IN, add the length of selected_genes list\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of informative genes:  1762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8SfyKLdnafy"
      },
      "source": [
        "Now, let's update the model to only include the genes which are informative, and see how it improves the accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWLZLOYAnaNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60285966-3a08-48d2-b41d-65172fed74ff"
      },
      "source": [
        "#This creates a data frame with only our selected genes\n",
        "X_filt = X.iloc[:, selected_genes]\n",
        "y_filt = y #Still the same y\n",
        "\n",
        "\n",
        "# Partition the dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_filt, y_filt, train_size=0.5, random_state=0)\n",
        "\n",
        "# Instantiate the classifiers\n",
        "# The parameters here are the default values for LogisticRegression,\n",
        "# included here just to suppress a couple warnings\n",
        "log_clf = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter = 100)\n",
        "\n",
        "# Train the classifiers\n",
        "log_clf.fit(X_train, y_train)\n",
        "\n",
        "# Calculate predictions on test set\n",
        "log_predictions = log_clf.predict(X_test)\n",
        "\n",
        "print(log_clf.score(X_train, y_train))\n",
        "print(log_clf.score(X_test, y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.9722222222222222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3l55TRSJNSg"
      },
      "source": [
        "Compare your accuracy to a classmates. If you get different numbers, why? Are you surprised at how well our classifier does? What do you think will happen if we get a larger data set?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qCeHlb8JbKf"
      },
      "source": [
        "DISCUSSION:"
      ]
    }
  ]
}